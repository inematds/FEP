# Production Systems

## O Que √â

Production Systems para LLMs s√£o arquiteturas robustas, escal√°veis e confi√°veis que levam aplica√ß√µes de IA generativa do prot√≥tipo para ambientes de produ√ß√£o com tr√°fego real, SLAs rigorosos e requisitos de neg√≥cio cr√≠ticos.

**Diferen√ßa entre Prot√≥tipo e Produ√ß√£o:**

| Aspecto | Prot√≥tipo | Produ√ß√£o |
|---------|-----------|----------|
| **Usu√°rios** | Voc√™ e time | Milhares/milh√µes |
| **Uptime** | "Funciona na minha m√°quina" | 99.9%+ SLA |
| **Lat√™ncia** | 10s? OK | <2s obrigat√≥rio |
| **Custos** | N√£o importa muito | Cada centavo conta |
| **Erros** | Refresh e tenta de novo | Graceful degradation |
| **Seguran√ßa** | API key no c√≥digo | Secrets management, auth |
| **Monitoring** | `console.log()` | Observabilidade completa |

## Por Que Usar

### Riscos de N√£o Preparar para Produ√ß√£o

1. **Indisponibilidade**
   - API da Anthropic com rate limit ‚Üí seu app quebra
   - Custo: usu√°rios frustrados, perda de receita

2. **Custos Explosivos**
   - Prompt mal otimizado √ó 1M chamadas = üí∏üí∏üí∏
   - Sem cache = pagar 10x mais que necess√°rio

3. **Performance Ruim**
   - Lat√™ncia de 15s ‚Üí usu√°rios abandonam
   - 3s de lat√™ncia = 32% mais bounce rate

4. **Seguran√ßa Vulner√°vel**
   - API keys vazadas ‚Üí conta comprometida
   - Prompt injection ‚Üí dados de clientes expostos

5. **Impossibilidade de Debug**
   - Bug em produ√ß√£o, zero logs ‚Üí "N√£o sei o que aconteceu"

### Benef√≠cios de Sistema de Produ√ß√£o S√≥lido

‚úÖ **Confiabilidade**: Funciona mesmo quando servi√ßos externos falham
‚úÖ **Escalabilidade**: Suporta crescimento de 100 ‚Üí 1M usu√°rios
‚úÖ **Observabilidade**: Voc√™ sabe exatamente o que est√° acontecendo
‚úÖ **Custo-efici√™ncia**: Otimiza√ß√µes salvam milhares de d√≥lares
‚úÖ **Manutenibilidade**: Deploys seguros, rollbacks f√°ceis

## Como Funciona

### Arquitetura de Refer√™ncia

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Load Balancer                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ               ‚îÇ               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   API Server ‚îÇ ‚îÇ API Server ‚îÇ ‚îÇ API Server ‚îÇ  (Auto-scaling)
‚îÇ   Instance 1 ‚îÇ ‚îÇ Instance 2 ‚îÇ ‚îÇ Instance N ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ               ‚îÇ               ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ               ‚îÇ                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Cache Layer   ‚îÇ ‚îÇ  Queue System   ‚îÇ ‚îÇ   Database   ‚îÇ
‚îÇ  (Redis/KV)    ‚îÇ ‚îÇ  (SQS/RabbitMQ) ‚îÇ ‚îÇ  (Postgres)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Worker Processes ‚îÇ  (Async jobs)
                    ‚îÇ  (LLM calls aqui) ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Anthropic API   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes Essenciais

#### 1. **Rate Limiting e Throttling**

```python
from functools import wraps
import time
from collections import deque

class RateLimiter:
    """Token bucket rate limiter"""
    def __init__(self, max_calls, time_window):
        self.max_calls = max_calls
        self.time_window = time_window
        self.calls = deque()

    def allow_request(self):
        now = time.time()

        # Remove chamadas antigas
        while self.calls and self.calls[0] < now - self.time_window:
            self.calls.popleft()

        if len(self.calls) < self.max_calls:
            self.calls.append(now)
            return True

        return False

# Decorator para rate limiting
limiter = RateLimiter(max_calls=50, time_window=60)  # 50 req/min

def rate_limited(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        if not limiter.allow_request():
            raise Exception("Rate limit exceeded. Try again later.")
        return func(*args, **kwargs)
    return wrapper

@rate_limited
def call_claude(prompt):
    # Sua chamada para API
    ...
```

#### 2. **Retry Logic com Exponential Backoff**

```python
import backoff
import anthropic

@backoff.on_exception(
    backoff.expo,  # Exponential backoff
    (
        anthropic.RateLimitError,
        anthropic.APIConnectionError,
        anthropic.InternalServerError
    ),
    max_tries=5,
    max_time=300  # M√°ximo 5 minutos tentando
)
def robust_claude_call(prompt):
    client = anthropic.Anthropic()

    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )

    return response.content[0].text

# Tenta automaticamente com delays: 1s, 2s, 4s, 8s, 16s
result = robust_claude_call("Hello")
```

#### 3. **Caching Agressivo**

```python
import hashlib
import redis
import json

class LLMCache:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379)
        self.ttl = 86400  # 24 horas

    def _get_cache_key(self, prompt, model, max_tokens):
        """Gera chave √∫nica baseada em par√¢metros"""
        key_data = f"{model}:{max_tokens}:{prompt}"
        return f"llm_cache:{hashlib.sha256(key_data.encode()).hexdigest()}"

    def get(self, prompt, model, max_tokens):
        """Busca no cache"""
        key = self._get_cache_key(prompt, model, max_tokens)
        cached = self.redis_client.get(key)

        if cached:
            print("Cache HIT!")
            return json.loads(cached)

        print("Cache MISS")
        return None

    def set(self, prompt, model, max_tokens, response):
        """Salva no cache"""
        key = self._get_cache_key(prompt, model, max_tokens)
        self.redis_client.setex(
            key,
            self.ttl,
            json.dumps(response)
        )

# Uso
cache = LLMCache()

def cached_claude_call(prompt):
    # Tenta cache primeiro
    cached = cache.get(prompt, "claude-sonnet-4", 1024)
    if cached:
        return cached

    # Cache miss - faz chamada real
    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )

    result = response.content[0].text

    # Salva no cache
    cache.set(prompt, "claude-sonnet-4", 1024, result)

    return result

# Economiza $$ em prompts repetidos
```

#### 4. **Circuit Breaker Pattern**

```python
from enum import Enum
import time

class CircuitState(Enum):
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, reject requests
    HALF_OPEN = "half_open"  # Testing if service recovered

class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

    def call(self, func, *args, **kwargs):
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.timeout:
                # Tenta novamente
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("Circuit breaker is OPEN. Service unavailable.")

        try:
            result = func(*args, **kwargs)

            # Sucesso - reset
            if self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.CLOSED
            self.failure_count = 0

            return result

        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()

            if self.failure_count >= self.failure_threshold:
                self.state = CircuitState.OPEN
                print(f"Circuit breaker OPENED after {self.failure_count} failures")

            raise e

# Uso
breaker = CircuitBreaker(failure_threshold=3, timeout=30)

def protected_claude_call(prompt):
    return breaker.call(robust_claude_call, prompt)

# Se 3 chamadas falharem, pr√≥ximas s√£o rejeitadas por 30s
# Evita sobrecarregar servi√ßo que j√° est√° com problemas
```

#### 5. **Async Processing com Filas**

```python
from celery import Celery
import anthropic

# Configura√ß√£o Celery (com Redis como broker)
app = Celery('tasks', broker='redis://localhost:6379/0')

@app.task(bind=True, max_retries=3)
def process_with_claude(self, user_input):
    """Task ass√≠ncrona para processar com Claude"""
    try:
        client = anthropic.Anthropic()

        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2048,
            messages=[{"role": "user", "content": user_input}]
        )

        return {
            "status": "success",
            "result": response.content[0].text
        }

    except Exception as e:
        # Retry com exponential backoff
        raise self.retry(exc=e, countdown=2 ** self.request.retries)

# API endpoint
from flask import Flask, jsonify, request

app_flask = Flask(__name__)

@app_flask.route('/api/analyze', methods=['POST'])
def analyze():
    user_input = request.json['input']

    # Enfileira tarefa (retorna imediatamente)
    task = process_with_claude.delay(user_input)

    return jsonify({
        "task_id": task.id,
        "status": "processing"
    }), 202

@app_flask.route('/api/result/<task_id>', methods=['GET'])
def get_result(task_id):
    task = process_with_claude.AsyncResult(task_id)

    if task.ready():
        return jsonify({
            "status": "completed",
            "result": task.result
        })
    else:
        return jsonify({
            "status": "processing"
        }), 202
```

## Exemplos Pr√°ticos

### Exemplo 1: API de Produ√ß√£o Completa

```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import anthropic
import redis
import logging
from prometheus_client import Counter, Histogram
import time

# M√©tricas
llm_requests = Counter('llm_requests_total', 'Total LLM requests')
llm_latency = Histogram('llm_latency_seconds', 'LLM request latency')
llm_errors = Counter('llm_errors_total', 'Total LLM errors')

# Setup
app = FastAPI()
cache = redis.Redis(host='redis', port=6379)
logger = logging.getLogger(__name__)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Em prod: lista espec√≠fica
    allow_methods=["*"],
    allow_headers=["*"],
)

class AnalysisRequest(BaseModel):
    text: str
    analysis_type: str = "sentiment"

class AnalysisResponse(BaseModel):
    result: str
    cached: bool
    latency_ms: float

@app.post("/api/v1/analyze", response_model=AnalysisResponse)
async def analyze_text(request: AnalysisRequest):
    """Endpoint de an√°lise com todas as pr√°ticas de produ√ß√£o"""

    start_time = time.time()
    llm_requests.inc()

    # 1. Valida√ß√£o
    if len(request.text) > 10000:
        raise HTTPException(400, "Text too long (max 10000 chars)")

    # 2. Cache lookup
    cache_key = f"analysis:{request.analysis_type}:{hash(request.text)}"
    cached_result = cache.get(cache_key)

    if cached_result:
        latency = (time.time() - start_time) * 1000
        return AnalysisResponse(
            result=cached_result.decode(),
            cached=True,
            latency_ms=latency
        )

    # 3. Rate limiting (por IP, user, etc)
    # ... implementa√ß√£o omitida

    # 4. LLM call com retry
    try:
        result = await call_claude_with_retry(request.text, request.analysis_type)
    except Exception as e:
        llm_errors.inc()
        logger.error(f"LLM call failed: {str(e)}")
        raise HTTPException(503, "Service temporarily unavailable")

    # 5. Cache result
    cache.setex(cache_key, 3600, result)  # 1 hora

    # 6. M√©tricas
    latency = (time.time() - start_time) * 1000
    llm_latency.observe(latency / 1000)

    return AnalysisResponse(
        result=result,
        cached=False,
        latency_ms=latency
    )

@app.get("/health")
async def health_check():
    """Health check para load balancer"""
    # Verifica dependencies
    try:
        cache.ping()
        # Testa Anthropic API
        client = anthropic.Anthropic()
        # Lightweight check (n√£o gasta cr√©ditos)
        return {"status": "healthy"}
    except:
        raise HTTPException(503, "Unhealthy")

async def call_claude_with_retry(text, analysis_type):
    """Chamada robusta ao Claude"""
    # Implementa√ß√£o com retry, circuit breaker, etc
    ...
```

### Exemplo 2: Gerenciamento de Custos

```python
class CostTracker:
    """Rastreia e controla custos de LLM"""

    PRICING = {
        "claude-opus-4-20250514": {
            "input": 0.015,   # per 1K tokens
            "output": 0.075
        },
        "claude-sonnet-4-20250514": {
            "input": 0.003,
            "output": 0.015
        },
        "claude-haiku-3-5-20250514": {
            "input": 0.0008,
            "output": 0.004
        }
    }

    def __init__(self, budget_limit_usd=1000):
        self.budget_limit = budget_limit_usd
        self.db = Database()  # Seu DB

    def track_usage(self, model, input_tokens, output_tokens):
        """Registra uso e calcula custo"""
        pricing = self.PRICING[model]

        cost = (
            (input_tokens / 1000) * pricing["input"] +
            (output_tokens / 1000) * pricing["output"]
        )

        self.db.insert_usage({
            "model": model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost_usd": cost,
            "timestamp": datetime.now()
        })

        return cost

    def check_budget(self):
        """Verifica se est√° dentro do budget"""
        total_cost = self.db.get_monthly_cost()

        if total_cost >= self.budget_limit:
            raise BudgetExceededException(
                f"Monthly budget of ${self.budget_limit} exceeded"
            )

        # Alert se > 80%
        if total_cost >= self.budget_limit * 0.8:
            self.send_alert(f"Budget at {(total_cost/self.budget_limit)*100:.1f}%")

    def optimize_model_selection(self, task_complexity):
        """Escolhe modelo mais custo-efetivo para a tarefa"""
        if task_complexity == "low":
            return "claude-haiku-3-5-20250514"  # Mais barato
        elif task_complexity == "medium":
            return "claude-sonnet-4-20250514"
        else:
            return "claude-opus-4-20250514"  # Mais caro, s√≥ quando necess√°rio

# Wrapper que rastreia custos
cost_tracker = CostTracker(budget_limit_usd=5000)

def cost_aware_claude_call(prompt, complexity="medium"):
    # Check budget
    cost_tracker.check_budget()

    # Escolhe modelo
    model = cost_tracker.optimize_model_selection(complexity)

    # Faz chamada
    response = client.messages.create(
        model=model,
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )

    # Rastreia uso
    cost_tracker.track_usage(
        model=model,
        input_tokens=response.usage.input_tokens,
        output_tokens=response.usage.output_tokens
    )

    return response.content[0].text
```

### Exemplo 3: Graceful Degradation

```python
class RobustLLMService:
    """Servi√ßo com fallbacks em cascata"""

    def __init__(self):
        self.primary_client = anthropic.Anthropic()
        self.cache = Cache()
        self.static_responses = self.load_static_responses()

    def process(self, user_input, context=None):
        # N√≠vel 1: Cache (mais r√°pido e barato)
        cached = self.cache.get(user_input)
        if cached:
            logger.info("Served from cache")
            return cached

        # N√≠vel 2: Primary LLM (Claude)
        try:
            result = self.call_primary_llm(user_input, context)
            self.cache.set(user_input, result)
            return result
        except anthropic.RateLimitError:
            logger.warning("Rate limited, trying fallback")
        except anthropic.APIError as e:
            logger.error(f"Primary LLM failed: {e}")

        # N√≠vel 3: Fallback LLM (modelo mais barato/alternativo)
        try:
            result = self.call_fallback_llm(user_input, context)
            return result
        except Exception as e:
            logger.error(f"Fallback LLM failed: {e}")

        # N√≠vel 4: Resposta est√°tica baseada em padr√µes
        pattern_response = self.match_static_pattern(user_input)
        if pattern_response:
            logger.warning("Serving static response")
            return pattern_response

        # N√≠vel 5: Erro amig√°vel
        return {
            "error": True,
            "message": "Service temporarily unavailable. Please try again later.",
            "retry_after": 60
        }

    def call_primary_llm(self, input, context):
        # Claude call
        ...

    def call_fallback_llm(self, input, context):
        # Modelo alternativo (GPT-4, etc) ou Claude Haiku
        ...

    def match_static_pattern(self, input):
        # Regex matching para perguntas comuns
        # "Qual o hor√°rio?" ‚Üí resposta est√°tica
        ...
```

## Casos de Uso Reais

### 1. **Chatbot de Atendimento (1M+ mensagens/m√™s)**

**Desafios:**
- Custo: $15k/m√™s em LLM calls
- Lat√™ncia: <2s obrigat√≥rio (SLA)
- Uptime: 99.9%

**Solu√ß√µes implementadas:**
- Cache de respostas similares: -70% de custos
- Queue para non-urgent requests
- Circuit breaker para Anthropic API
- Fallback para respostas template

**Resultado:** Custo reduzido para $4k/m√™s, lat√™ncia p95 = 1.2s

### 2. **An√°lise de Documentos Legais**

**Desafios:**
- Documentos de 50-200 p√°ginas
- Processamento pode levar minutos
- Usu√°rios n√£o podem esperar s√≠ncrono

**Solu√ß√£o:**
- Async processing com Celery
- Progress updates via WebSocket
- Partial results (streaming)
- Persistent storage dos resultados

### 3. **Code Review Automation**

**Desafios:**
- PRs com 1000+ linhas de c√≥digo
- Precisa rodar em CI/CD pipeline
- Budget limitado ($500/m√™s)

**Solu√ß√£o:**
- Model selection din√¢mico:
  - Haiku para syntax/style (barato)
  - Sonnet para logic bugs (m√©dio)
  - Opus s√≥ para security (cr√≠tico)
- Cache de an√°lises de arquivos n√£o modificados
- Parallel processing de arquivos

## Dicas Pr√°ticas

### 1. **Deployment Strategy**

```yaml
# Blue-Green Deployment
# docker-compose.yml

version: '3'
services:
  # Green (vers√£o atual)
  api-green:
    image: myapp:v1.0
    environment:
      - VERSION=v1.0
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.api.rule=Host(`api.example.com`)"
      - "traefik.http.services.api.loadbalancer.server.port=8000"
      - "traefik.http.services.api.loadbalancer.healthcheck.path=/health"

  # Blue (nova vers√£o)
  api-blue:
    image: myapp:v1.1
    environment:
      - VERSION=v1.1
    labels:
      - "traefik.enable=false"  # Come√ßa desabilitado

# Deploy: habilita blue, monitora, desabilita green se OK
```

### 2. **Feature Flags**

```python
from launchdarkly import LDClient, Config

ld_client = LDClient(config=Config("your-sdk-key"))

def call_llm(prompt, user_id):
    # Feature flag: testar novo modelo com 10% dos usu√°rios
    use_new_model = ld_client.variation(
        "use-claude-opus-4",
        {"key": user_id},
        default=False
    )

    model = "claude-opus-4-20250514" if use_new_model else "claude-sonnet-4-20250514"

    response = client.messages.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )

    return response.content[0].text

# Rollout gradual: 10% ‚Üí 50% ‚Üí 100%
```

### 3. **Database Design para Logs**

```sql
-- Tabela de logs otimizada
CREATE TABLE llm_requests (
    id BIGSERIAL PRIMARY KEY,
    request_id UUID NOT NULL,
    user_id VARCHAR(100),
    model VARCHAR(50) NOT NULL,
    prompt_hash VARCHAR(64),  -- SHA256 do prompt (n√£o armazena prompt completo por privacidade)
    input_tokens INT,
    output_tokens INT,
    cost_usd DECIMAL(10, 6),
    latency_ms INT,
    status VARCHAR(20),  -- success, error, timeout
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- √çndices para queries comuns
CREATE INDEX idx_user_id_created ON llm_requests(user_id, created_at DESC);
CREATE INDEX idx_created_at ON llm_requests(created_at DESC);
CREATE INDEX idx_model_status ON llm_requests(model, status);

-- Particionamento por m√™s (para volumes altos)
CREATE TABLE llm_requests_2024_01 PARTITION OF llm_requests
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
```

### 4. **Secrets Management**

```python
# N√ÉO FA√áA ISSO:
api_key = "sk-ant-api03-abc123..."  # ‚ùå Hardcoded

# FA√áA ISSO:
import os
from google.cloud import secretmanager

def get_secret(secret_name):
    """Busca secret do Google Secret Manager"""
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{PROJECT_ID}/secrets/{secret_name}/versions/latest"
    response = client.access_secret_version(request={"name": name})
    return response.payload.data.decode("UTF-8")

# Uso
ANTHROPIC_API_KEY = get_secret("anthropic-api-key")
client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

# Ou use vari√°veis de ambiente (m√≠nimo)
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
if not ANTHROPIC_API_KEY:
    raise Exception("API key not configured")
```

## Erros Comuns e Como Evitar

### ‚ùå Erro 1: Sem Timeout

**Problema:**
```python
# Request pode travar para sempre
response = client.messages.create(...)  # Espera indefinidamente
```

### ‚úÖ Solu√ß√£o: Sempre Use Timeout

```python
import httpx

# Configure timeout no client
client = anthropic.Anthropic(
    timeout=httpx.Timeout(60.0, connect=5.0)  # 60s total, 5s para conectar
)

# Ou por request
response = client.messages.create(
    ...,
    timeout=30.0  # 30 segundos
)
```

### ‚ùå Erro 2: Logs Inseguros

**Problema:**
```python
# Loga dados sens√≠veis
logger.info(f"User {user_id} asked: {prompt}")  # Prompt pode ter PII!
logger.info(f"Response: {response}")  # Response pode ter dados confidenciais
```

### ‚úÖ Solu√ß√£o: Log Sanitization

```python
import hashlib

def sanitize_for_logging(text, max_length=100):
    """Remove PII e trunca para logging"""
    # Hash ao inv√©s de texto completo
    text_hash = hashlib.sha256(text.encode()).hexdigest()[:12]

    # Log apenas metadata
    return {
        "hash": text_hash,
        "length": len(text),
        "preview": text[:max_length] + "..." if len(text) > max_length else text
    }

logger.info(f"Request: {sanitize_for_logging(prompt)}")
```

### ‚ùå Erro 3: Single Point of Failure

**Problema:**
```python
# Se Anthropic API cair, app inteiro para
def process(input):
    return client.messages.create(...)  # Sem fallback
```

### ‚úÖ Solu√ß√£o: Redund√¢ncia

```python
def process_with_redundancy(input):
    providers = [
        ("anthropic", call_claude),
        ("openai", call_gpt4),
        ("local", call_local_llm)  # Fallback final
    ]

    for provider_name, provider_func in providers:
        try:
            result = provider_func(input)
            logger.info(f"Succeeded with {provider_name}")
            return result
        except Exception as e:
            logger.warning(f"{provider_name} failed: {e}")
            continue

    raise Exception("All providers failed")
```

## Recursos Adicionais

### Ferramentas e Frameworks
- **[FastAPI](https://fastapi.tiangolo.com/)** - Framework web moderno para Python
- **[Celery](https://docs.celeryq.dev/)** - Distributed task queue
- **[Redis](https://redis.io/)** - Cache e message broker
- **[Prometheus](https://prometheus.io/)** + **[Grafana](https://grafana.com/)** - Monitoring
- **[Sentry](https://sentry.io/)** - Error tracking

### Cloud Providers
- **AWS**: Lambda + SQS + ElastiCache
- **GCP**: Cloud Run + Pub/Sub + Memorystore
- **Azure**: Functions + Service Bus + Redis

### Checklists
- [Production Readiness Checklist](https://docs.anthropic.com/claude/docs/production-readiness)
- [12 Factor App](https://12factor.net/) - Metodologia para apps cloud-native

### Exemplos
- [Claude Production Starter](https://github.com/anthropics/claude-production-starter)
- [LLM Production Patterns](https://github.com/examples/llm-production)
